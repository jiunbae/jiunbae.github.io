{"componentChunkName":"component---src-templates-post-tsx","path":"/posts/ai-model-scaling-strategy-2021/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"2021년-ai-모델-스케일링-전략과-경험\">2021년 AI 모델 스케일링 전략과 경험</h1>\n<p>2021년은 AI 분야에서 \"더 크게\"가 화두였습니다. GPT-3가 175B 파라미터로 인상적인 성능을 보여준 이후, 모델 크기 경쟁이 본격화되었습니다. 저도 회사에서 10B 규모의 모델을 학습시켜본 경험이 있어서, 그 과정에서 배운 것들을 정리해보았습니다.</p>\n<h2 id=\"스케일링의-세-가지-축\">스케일링의 세 가지 축</h2>\n<p>대규모 모델을 학습시키기 위한 분산 학습 방법은 크게 세 가지로 나눌 수 있습니다.</p>\n<h3 id=\"1-데이터-병렬화-data-parallelism\">1. 데이터 병렬화 (Data Parallelism)</h3>\n<p>가장 기본적이고 널리 쓰이는 방법입니다. 동일한 모델을 여러 GPU에 복제하고, 데이터를 나눠서 처리합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>distributed <span class=\"token keyword\">as</span> dist\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>parallel <span class=\"token keyword\">import</span> DistributedDataParallel <span class=\"token keyword\">as</span> DDP\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">setup</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    dist<span class=\"token punctuation\">.</span>init_process_group<span class=\"token punctuation\">(</span>\n        backend<span class=\"token operator\">=</span><span class=\"token string\">'nccl'</span><span class=\"token punctuation\">,</span>\n        init_method<span class=\"token operator\">=</span><span class=\"token string\">'env://'</span><span class=\"token punctuation\">,</span>\n        world_size<span class=\"token operator\">=</span>world_size<span class=\"token punctuation\">,</span>\n        rank<span class=\"token operator\">=</span>rank\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">train_with_ddp</span><span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    setup<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">,</span> world_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># 모델을 현재 GPU로 이동</span>\n    model <span class=\"token operator\">=</span> MyModel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># DDP로 래핑</span>\n    model <span class=\"token operator\">=</span> DDP<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> device_ids<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>rank<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># 데이터 로더 (각 GPU가 다른 데이터를 받음)</span>\n    sampler <span class=\"token operator\">=</span> DistributedSampler<span class=\"token punctuation\">(</span>dataset<span class=\"token punctuation\">,</span> num_replicas<span class=\"token operator\">=</span>world_size<span class=\"token punctuation\">,</span> rank<span class=\"token operator\">=</span>rank<span class=\"token punctuation\">)</span>\n    loader <span class=\"token operator\">=</span> DataLoader<span class=\"token punctuation\">(</span>dataset<span class=\"token punctuation\">,</span> sampler<span class=\"token operator\">=</span>sampler<span class=\"token punctuation\">,</span> batch_size<span class=\"token operator\">=</span><span class=\"token number\">32</span><span class=\"token punctuation\">)</span>\n\n    optimizer <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>AdamW<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">1e-4</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">for</span> epoch <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>num_epochs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        sampler<span class=\"token punctuation\">.</span>set_epoch<span class=\"token punctuation\">(</span>epoch<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 셔플링을 위해 필요</span>\n        <span class=\"token keyword\">for</span> batch <span class=\"token keyword\">in</span> loader<span class=\"token punctuation\">:</span>\n            optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n            loss <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">)</span>\n            loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n            optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 실행</span>\n<span class=\"token keyword\">if</span> __name__ <span class=\"token operator\">==</span> <span class=\"token string\">\"__main__\"</span><span class=\"token punctuation\">:</span>\n    world_size <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>device_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    torch<span class=\"token punctuation\">.</span>multiprocessing<span class=\"token punctuation\">.</span>spawn<span class=\"token punctuation\">(</span>train_with_ddp<span class=\"token punctuation\">,</span> args<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>world_size<span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> nprocs<span class=\"token operator\">=</span>world_size<span class=\"token punctuation\">)</span></code></pre></div>\n<p>PyTorch의 <code class=\"language-text\">DistributedDataParallel</code>(DDP)을 사용하면 비교적 쉽게 구현할 수 있습니다. 각 GPU가 gradient를 계산하고, all-reduce 연산으로 gradient를 평균 내어 동기화합니다.</p>\n<p><strong>장점</strong>: 구현이 간단하고 효율적입니다.\n<strong>단점</strong>: 모델 전체가 각 GPU에 올라가야 하므로, GPU 메모리보다 큰 모델은 불가능합니다.</p>\n<h3 id=\"2-모델-병렬화-model-parallelism\">2. 모델 병렬화 (Model Parallelism)</h3>\n<p>모델 자체를 여러 GPU에 나눠서 배치하는 방법입니다. 하나의 GPU에 올라가지 않는 거대 모델을 학습시킬 때 필수입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">ModelParallelTransformer</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> num_layers<span class=\"token operator\">=</span><span class=\"token number\">24</span><span class=\"token punctuation\">,</span> d_model<span class=\"token operator\">=</span><span class=\"token number\">1024</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># 레이어를 두 GPU에 나눔</span>\n        self<span class=\"token punctuation\">.</span>embedding <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span><span class=\"token number\">50000</span><span class=\"token punctuation\">,</span> d_model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:0'</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># 전반부 레이어 → GPU 0</span>\n        self<span class=\"token punctuation\">.</span>layers_gpu0 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>ModuleList<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n            TransformerLayer<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>num_layers <span class=\"token operator\">//</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:0'</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># 후반부 레이어 → GPU 1</span>\n        self<span class=\"token punctuation\">.</span>layers_gpu1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>ModuleList<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n            TransformerLayer<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> _ <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>num_layers <span class=\"token operator\">//</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span>\n\n        self<span class=\"token punctuation\">.</span>output <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">,</span> <span class=\"token number\">50000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># GPU 0에서 처리</span>\n        x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:0'</span><span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>embedding<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">for</span> layer <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>layers_gpu0<span class=\"token punctuation\">:</span>\n            x <span class=\"token operator\">=</span> layer<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># GPU 1로 이동 후 처리</span>\n        x <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda:1'</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">for</span> layer <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>layers_gpu1<span class=\"token punctuation\">:</span>\n            x <span class=\"token operator\">=</span> layer<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>output<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>장점</strong>: 단일 GPU 메모리보다 큰 모델을 학습할 수 있습니다.\n<strong>단점</strong>: GPU 간 통신 오버헤드가 크고, GPU 활용률이 떨어집니다.</p>\n<h3 id=\"3-파이프라인-병렬화-pipeline-parallelism\">3. 파이프라인 병렬화 (Pipeline Parallelism)</h3>\n<p>모델 병렬화의 GPU 활용률 문제를 개선한 방법입니다. 배치를 마이크로 배치로 나누고, 파이프라인처럼 처리합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 개념적인 파이프라인 병렬화 설명</span>\n<span class=\"token triple-quoted-string string\">\"\"\"\n기존 모델 병렬화:\nTime →  [GPU0: Layer 1-12]  [GPU1: Layer 13-24]\nBatch 1:    ████████████████    ████████████████\n           └─ GPU1은 GPU0이 끝날 때까지 대기 (낭비)\n\n파이프라인 병렬화:\nTime →\nGPU0:  [μB1][μB2][μB3][μB4]  ← 마이크로 배치들\nGPU1:       [μB1][μB2][μB3][μB4]\n           └─ 이전 마이크로 배치 처리와 병렬 실행\n\"\"\"</span>\n\n<span class=\"token comment\"># PyTorch의 pipeline 예시 (간략화)</span>\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>distributed<span class=\"token punctuation\">.</span>pipeline<span class=\"token punctuation\">.</span>sync <span class=\"token keyword\">import</span> Pipe\n\nmodel <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span>\n    nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">1024</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1024</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># Layer 1</span>\n    nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">1024</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1024</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># Layer 2</span>\n    <span class=\"token comment\"># ... 더 많은 레이어</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 2개 GPU에 파이프라인으로 분할</span>\nmodel <span class=\"token operator\">=</span> Pipe<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> chunks<span class=\"token operator\">=</span><span class=\"token number\">8</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 8개 마이크로 배치로 분할</span></code></pre></div>\n<p><strong>장점</strong>: GPU 활용률이 모델 병렬화보다 높습니다.\n<strong>단점</strong>: 구현이 복잡하고, bubble (idle time)이 여전히 존재합니다.</p>\n<h2 id=\"deepspeed와-zero\">DeepSpeed와 ZeRO</h2>\n<p>2021년에 분산 학습의 판도를 바꾼 것은 Microsoft의 <a href=\"https://www.deepspeed.ai/\">DeepSpeed</a>였습니다. 특히 ZeRO(Zero Redundancy Optimizer) 기술이 인상적이었습니다.</p>\n<h3 id=\"zero의-핵심-아이디어\">ZeRO의 핵심 아이디어</h3>\n<p>기존 데이터 병렬화에서는 각 GPU가 모델 전체의 복사본을 가지고 있습니다. 여기에는 세 가지가 포함됩니다:</p>\n<ul>\n<li><strong>모델 파라미터 (Parameters)</strong></li>\n<li><strong>그래디언트 (Gradients)</strong></li>\n<li><strong>옵티마이저 상태 (Optimizer States)</strong></li>\n</ul>\n<p>Adam 옵티마이저의 경우, 옵티마이저 상태가 파라미터의 2배 메모리를 차지합니다. 즉, FP16 모델 기준으로:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">메모리 사용량 (GPU당):\n- 파라미터: 2 bytes × N\n- 그래디언트: 2 bytes × N\n- 옵티마이저 상태 (Adam): 4 bytes × N + 4 bytes × N = 8 bytes × N\n총: 12 bytes × N (파라미터 수)\n\n10B 모델 기준:\n10B × 12 bytes = 120GB per GPU\n→ A100 80GB로도 불가능</code></pre></div>\n<p>ZeRO는 이 상태들을 GPU 간에 분산하여 중복을 제거합니다:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># DeepSpeed ZeRO Stage 3 사용 예시</span>\n<span class=\"token keyword\">import</span> deepspeed\n<span class=\"token keyword\">import</span> torch\n\n<span class=\"token comment\"># DeepSpeed 설정 파일 (ds_config.json)</span>\nds_config <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token string\">\"train_batch_size\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">256</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"gradient_accumulation_steps\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">16</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"fp16\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token string\">\"enabled\"</span><span class=\"token punctuation\">:</span> <span class=\"token boolean\">True</span>\n    <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"zero_optimization\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token string\">\"stage\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># ZeRO Stage 3: 파라미터, 그래디언트, 옵티마이저 상태 모두 분산</span>\n        <span class=\"token string\">\"offload_optimizer\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span>\n            <span class=\"token string\">\"device\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"cpu\"</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># 옵티마이저 상태를 CPU로 오프로드</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"offload_param\"</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span>\n            <span class=\"token string\">\"device\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"cpu\"</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># 파라미터도 CPU로 오프로드 가능</span>\n        <span class=\"token punctuation\">}</span>\n    <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token comment\"># 모델 초기화</span>\nmodel <span class=\"token operator\">=</span> MyLargeModel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># DeepSpeed로 래핑</span>\nmodel_engine<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">,</span> _<span class=\"token punctuation\">,</span> _ <span class=\"token operator\">=</span> deepspeed<span class=\"token punctuation\">.</span>initialize<span class=\"token punctuation\">(</span>\n    model<span class=\"token operator\">=</span>model<span class=\"token punctuation\">,</span>\n    model_parameters<span class=\"token operator\">=</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    config<span class=\"token operator\">=</span>ds_config\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 학습 루프</span>\n<span class=\"token keyword\">for</span> batch <span class=\"token keyword\">in</span> dataloader<span class=\"token punctuation\">:</span>\n    loss <span class=\"token operator\">=</span> model_engine<span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">)</span>\n    model_engine<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span>loss<span class=\"token punctuation\">)</span>\n    model_engine<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>ZeRO Stage 3를 사용하면 메모리 효율이 크게 향상됩니다:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">ZeRO Stage 3 메모리 사용량 (N개 GPU):\n- 파라미터: 2 bytes × N / N = 2 bytes × N (통신 시에만 수집)\n- 그래디언트: 2 bytes × N / N\n- 옵티마이저 상태: 8 bytes × N / N\n\n10B 모델, 8 GPU 기준:\n(2 + 2 + 8) × 10B / 8 = 15GB per GPU\n→ A100 80GB로 충분히 가능</code></pre></div>\n<h2 id=\"직접-10b-모델을-학습시켜-본-경험\">직접 10B 모델을 학습시켜 본 경험</h2>\n<p>회사에서 10B 규모의 모델을 학습시켜봤습니다. A100 8대로 약 2주가 걸렸습니다.</p>\n<h3 id=\"환경-구성\">환경 구성</h3>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token comment\"># NVIDIA 컨테이너 사용</span>\n<span class=\"token function\">docker</span> pull nvcr.io/nvidia/pytorch:21.08-py3\n\n<span class=\"token comment\"># 노드 간 통신을 위한 NCCL 설정</span>\n<span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">NCCL_DEBUG</span><span class=\"token operator\">=</span>INFO\n<span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">NCCL_IB_DISABLE</span><span class=\"token operator\">=</span><span class=\"token number\">0</span>\n<span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">NCCL_NET_GDR_LEVEL</span><span class=\"token operator\">=</span><span class=\"token number\">5</span>\n\n<span class=\"token comment\"># DeepSpeed 실행</span>\ndeepspeed <span class=\"token parameter variable\">--num_gpus</span><span class=\"token operator\">=</span><span class=\"token number\">8</span> <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--hostfile</span><span class=\"token operator\">=</span>hostfile <span class=\"token punctuation\">\\</span>\n    train.py <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--deepspeed_config</span> ds_config.json</code></pre></div>\n<h3 id=\"겪었던-문제들\">겪었던 문제들</h3>\n<p><strong>1. 분산 학습 디버깅의 어려움</strong></p>\n<p>분산 학습에서 버그가 발생하면 디버깅이 매우 어렵습니다. 어떤 노드에서 문제가 생겼는지 파악하기 힘들고, 재현도 어렵습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 분산 환경에서의 디버깅 팁</span>\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>distributed <span class=\"token keyword\">as</span> dist\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">debug_log</span><span class=\"token punctuation\">(</span>message<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    rank <span class=\"token operator\">=</span> dist<span class=\"token punctuation\">.</span>get_rank<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">if</span> dist<span class=\"token punctuation\">.</span>is_initialized<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">else</span> <span class=\"token number\">0</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"[Rank </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>rank<span class=\"token punctuation\">}</span></span><span class=\"token string\">] </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>message<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 체크포인트를 자주 저장</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">save_checkpoint</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">,</span> epoch<span class=\"token punctuation\">,</span> loss<span class=\"token punctuation\">,</span> path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> dist<span class=\"token punctuation\">.</span>get_rank<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>  <span class=\"token comment\"># 마스터 노드에서만 저장</span>\n        torch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span>\n            <span class=\"token string\">'epoch'</span><span class=\"token punctuation\">:</span> epoch<span class=\"token punctuation\">,</span>\n            <span class=\"token string\">'model_state_dict'</span><span class=\"token punctuation\">:</span> model<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            <span class=\"token string\">'optimizer_state_dict'</span><span class=\"token punctuation\">:</span> optimizer<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            <span class=\"token string\">'loss'</span><span class=\"token punctuation\">:</span> loss<span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span> path<span class=\"token punctuation\">)</span>\n    dist<span class=\"token punctuation\">.</span>barrier<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 모든 노드가 저장 완료를 기다림</span></code></pre></div>\n<p><strong>2. OOM (Out of Memory) 문제</strong></p>\n<p>처음에는 배치 사이즈를 너무 크게 잡아서 OOM이 자주 발생했습니다. Gradient accumulation으로 해결했습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Gradient Accumulation 예시</span>\naccumulation_steps <span class=\"token operator\">=</span> <span class=\"token number\">16</span>\noptimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span> batch <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>dataloader<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    loss <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> accumulation_steps\n    loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>i <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">%</span> accumulation_steps <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>3. 학습 불안정성</strong></p>\n<p>대규모 모델은 학습이 불안정해지기 쉽습니다. Loss가 갑자기 발산하는 경우가 있었습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 학습 안정화를 위한 기법들</span>\n\n<span class=\"token comment\"># 1. Gradient Clipping</span>\ntorch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>clip_grad_norm_<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> max_norm<span class=\"token operator\">=</span><span class=\"token number\">1.0</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 2. Learning Rate Warmup</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">get_lr</span><span class=\"token punctuation\">(</span>step<span class=\"token punctuation\">,</span> warmup_steps<span class=\"token operator\">=</span><span class=\"token number\">1000</span><span class=\"token punctuation\">,</span> max_lr<span class=\"token operator\">=</span><span class=\"token number\">1e-4</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> step <span class=\"token operator\">&lt;</span> warmup_steps<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> max_lr <span class=\"token operator\">*</span> step <span class=\"token operator\">/</span> warmup_steps\n    <span class=\"token keyword\">return</span> max_lr\n\n<span class=\"token comment\"># 3. Loss Scaling (FP16 학습 시)</span>\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>amp <span class=\"token keyword\">import</span> GradScaler<span class=\"token punctuation\">,</span> autocast\n\nscaler <span class=\"token operator\">=</span> GradScaler<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">with</span> autocast<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    loss <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">)</span>\n\nscaler<span class=\"token punctuation\">.</span>scale<span class=\"token punctuation\">(</span>loss<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nscaler<span class=\"token punctuation\">.</span>unscale_<span class=\"token punctuation\">(</span>optimizer<span class=\"token punctuation\">)</span>\ntorch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>clip_grad_norm_<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> max_norm<span class=\"token operator\">=</span><span class=\"token number\">1.0</span><span class=\"token punctuation\">)</span>\nscaler<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span>optimizer<span class=\"token punctuation\">)</span>\nscaler<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3 id=\"체크포인트-관리의-중요성\">체크포인트 관리의 중요성</h3>\n<p>10B 모델의 체크포인트는 용량이 매우 큽니다. 체크포인트 관리 전략이 중요했습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 체크포인트 저장 전략</span>\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">CheckpointManager</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> save_dir<span class=\"token punctuation\">,</span> max_checkpoints<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>save_dir <span class=\"token operator\">=</span> save_dir\n        self<span class=\"token punctuation\">.</span>max_checkpoints <span class=\"token operator\">=</span> max_checkpoints\n        self<span class=\"token punctuation\">.</span>checkpoints <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">save</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">,</span> optimizer<span class=\"token punctuation\">,</span> epoch<span class=\"token punctuation\">,</span> loss<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        path <span class=\"token operator\">=</span> <span class=\"token string-interpolation\"><span class=\"token string\">f\"</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>self<span class=\"token punctuation\">.</span>save_dir<span class=\"token punctuation\">}</span></span><span class=\"token string\">/checkpoint_epoch</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>epoch<span class=\"token punctuation\">}</span></span><span class=\"token string\">.pt\"</span></span>\n\n        <span class=\"token comment\"># 저장</span>\n        torch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span>\n            <span class=\"token string\">'epoch'</span><span class=\"token punctuation\">:</span> epoch<span class=\"token punctuation\">,</span>\n            <span class=\"token string\">'model_state_dict'</span><span class=\"token punctuation\">:</span> model<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            <span class=\"token string\">'optimizer_state_dict'</span><span class=\"token punctuation\">:</span> optimizer<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            <span class=\"token string\">'loss'</span><span class=\"token punctuation\">:</span> loss<span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span> path<span class=\"token punctuation\">)</span>\n\n        self<span class=\"token punctuation\">.</span>checkpoints<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>path<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># 오래된 체크포인트 삭제 (최근 N개만 유지)</span>\n        <span class=\"token keyword\">while</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>checkpoints<span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> self<span class=\"token punctuation\">.</span>max_checkpoints<span class=\"token punctuation\">:</span>\n            old_path <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>checkpoints<span class=\"token punctuation\">.</span>pop<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">if</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>exists<span class=\"token punctuation\">(</span>old_path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                os<span class=\"token punctuation\">.</span>remove<span class=\"token punctuation\">(</span>old_path<span class=\"token punctuation\">)</span></code></pre></div>\n<h2 id=\"혼합-정밀도-학습-mixed-precision-training\">혼합 정밀도 학습 (Mixed Precision Training)</h2>\n<p>FP16 학습은 2021년에 거의 표준이 되었습니다. 메모리를 절반으로 줄이면서 속도도 빨라집니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>amp <span class=\"token keyword\">import</span> autocast<span class=\"token punctuation\">,</span> GradScaler\n\n<span class=\"token comment\"># Mixed Precision Training 기본 패턴</span>\nscaler <span class=\"token operator\">=</span> GradScaler<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> batch <span class=\"token keyword\">in</span> dataloader<span class=\"token punctuation\">:</span>\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># autocast 블록 내에서 연산은 자동으로 FP16으로 수행</span>\n    <span class=\"token keyword\">with</span> autocast<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">)</span>\n        loss <span class=\"token operator\">=</span> criterion<span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">,</span> target<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Gradient scaling으로 언더플로우 방지</span>\n    scaler<span class=\"token punctuation\">.</span>scale<span class=\"token punctuation\">(</span>loss<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Gradient unscale 후 클리핑</span>\n    scaler<span class=\"token punctuation\">.</span>unscale_<span class=\"token punctuation\">(</span>optimizer<span class=\"token punctuation\">)</span>\n    torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>clip_grad_norm_<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> max_norm<span class=\"token operator\">=</span><span class=\"token number\">1.0</span><span class=\"token punctuation\">)</span>\n\n    scaler<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span>optimizer<span class=\"token punctuation\">)</span>\n    scaler<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>주의할 점:</p>\n<ul>\n<li><strong>Loss Scaling</strong>: FP16의 좁은 표현 범위로 인한 gradient 언더플로우를 방지합니다.</li>\n<li><strong>특정 연산은 FP32 유지</strong>: LayerNorm, Softmax 등은 FP32로 계산해야 수치 안정성이 보장됩니다.</li>\n</ul>\n<h2 id=\"2021년의-교훈\">2021년의 교훈</h2>\n<h3 id=\"효율성이-우선이다\">효율성이 우선이다</h3>\n<p>무작정 자원을 늘리는 것보다 최적화가 먼저입니다. DeepSpeed의 ZeRO, Gradient checkpointing, Mixed precision 등의 기법으로 같은 하드웨어에서 훨씬 큰 모델을 학습시킬 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Gradient Checkpointing 예시</span>\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>checkpoint <span class=\"token keyword\">import</span> checkpoint\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">EfficientTransformerLayer</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> d_model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>attention <span class=\"token operator\">=</span> MultiHeadAttention<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>ffn <span class=\"token operator\">=</span> FeedForward<span class=\"token punctuation\">(</span>d_model<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># checkpoint로 메모리 절약 (연산은 재계산하지만 메모리는 절약)</span>\n        x <span class=\"token operator\">=</span> x <span class=\"token operator\">+</span> checkpoint<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>attention<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span>\n        x <span class=\"token operator\">=</span> x <span class=\"token operator\">+</span> checkpoint<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>ffn<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> x</code></pre></div>\n<h3 id=\"빅테크와의-격차\">빅테크와의 격차</h3>\n<p>솔직히, GPU 수천 개를 돌릴 수 있는 곳과 그렇지 않은 곳의 격차가 느껴졌습니다. GPT-3급 모델을 처음부터 학습시키는 것은 대부분의 조직에서 불가능합니다.</p>\n<p>하지만 오픈소스 모델들이 나오면서 이 격차가 조금씩 줄고 있습니다. 직접 학습시키지 않아도 공개된 모델을 fine-tuning하여 활용할 수 있게 되었습니다.</p>\n<h3 id=\"인프라-엔지니어링의-중요성\">인프라 엔지니어링의 중요성</h3>\n<p>대규모 모델 학습에서는 ML 알고리즘만큼이나 인프라 엔지니어링이 중요합니다. 분산 시스템, 네트워크 최적화, 스토리지 관리 등의 역량이 필요합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token comment\"># 학습 모니터링 스크립트 예시</span>\n<span class=\"token comment\">#!/bin/bash</span>\n\n<span class=\"token comment\"># GPU 상태 모니터링</span>\n<span class=\"token function\">watch</span> <span class=\"token parameter variable\">-n</span> <span class=\"token number\">1</span> nvidia-smi\n\n<span class=\"token comment\"># 노드 간 네트워크 대역폭 확인</span>\niperf3 <span class=\"token parameter variable\">-c</span> other_node <span class=\"token parameter variable\">-p</span> <span class=\"token number\">5001</span>\n\n<span class=\"token comment\"># 학습 로그 실시간 확인</span>\n<span class=\"token function\">tail</span> <span class=\"token parameter variable\">-f</span> logs/training.log <span class=\"token operator\">|</span> <span class=\"token function\">grep</span> <span class=\"token parameter variable\">-E</span> <span class=\"token string\">\"(loss|step|lr)\"</span></code></pre></div>\n<h2 id=\"마치며\">마치며</h2>\n<p>2021년은 스케일링의 해였습니다. \"모델을 크게 만들면 성능이 좋아진다\"는 단순한 원리가 실제로 동작한다는 것이 확인되었고, 이를 가능하게 하는 분산 학습 기술들이 빠르게 발전했습니다.</p>\n<p>직접 대규모 모델을 학습시켜본 경험은 소중했습니다. 논문에서 읽는 것과 실제로 해보는 것은 완전히 다른 차원의 이해를 제공합니다. 분산 학습의 어려움, 최적화의 중요성, 그리고 인프라의 가치를 직접 체감할 수 있었습니다.</p>\n<h2 id=\"참고-자료\">참고 자료</h2>\n<ul>\n<li><a href=\"https://www.deepspeed.ai/docs/\">DeepSpeed Documentation</a></li>\n<li><a href=\"https://arxiv.org/abs/1910.02054\">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a></li>\n<li><a href=\"https://arxiv.org/abs/1909.08053\">Megatron-LM: Training Multi-Billion Parameter Language Models</a></li>\n<li><a href=\"https://pytorch.org/docs/stable/distributed.html\">PyTorch Distributed Documentation</a></li>\n</ul>","excerpt":"2021년 AI 모델 스케일링 전략과 경험 2021년은 AI 분야에서 \"더 크게\"가 화두였습니다. GPT-3가 175B 파라미터로 인상적인 성능을 보여준 이후, 모델 크기 경쟁이 본격화되었습니다. 저도 회사에서 10B 규모의 모델을 학습시켜본 경험이 있어서, 그 과정에서 배운 것들을 정리해보았습니다. 스케일링의 세 가지 축 대규모 모델을 학습시키기 위한 …","frontmatter":{"date":"21.09.01","dateISO":"2021-09-01","description":"GPT-3 이후 대규모 모델 학습에 대해 배운 것들","slug":"/ai-model-scaling-strategy-2021","heroImage":null,"heroImageAlt":null,"tags":["ai","dev"],"title":"2021년 AI 모델 스케일링 전략과 경험"},"tableOfContents":"<ul>\n<li>\n<p><a href=\"#2021%EB%85%84-ai-%EB%AA%A8%EB%8D%B8-%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81-%EC%A0%84%EB%9E%B5%EA%B3%BC-%EA%B2%BD%ED%97%98\">2021년 AI 모델 스케일링 전략과 경험</a></p>\n<ul>\n<li>\n<p><a href=\"#%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81%EC%9D%98-%EC%84%B8-%EA%B0%80%EC%A7%80-%EC%B6%95\">스케일링의 세 가지 축</a></p>\n<ul>\n<li><a href=\"#1-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B3%91%EB%A0%AC%ED%99%94-data-parallelism\">1. 데이터 병렬화 (Data Parallelism)</a></li>\n<li><a href=\"#2-%EB%AA%A8%EB%8D%B8-%EB%B3%91%EB%A0%AC%ED%99%94-model-parallelism\">2. 모델 병렬화 (Model Parallelism)</a></li>\n<li><a href=\"#3-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EB%B3%91%EB%A0%AC%ED%99%94-pipeline-parallelism\">3. 파이프라인 병렬화 (Pipeline Parallelism)</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#deepspeed%EC%99%80-zero\">DeepSpeed와 ZeRO</a></p>\n<ul>\n<li><a href=\"#zero%EC%9D%98-%ED%95%B5%EC%8B%AC-%EC%95%84%EC%9D%B4%EB%94%94%EC%96%B4\">ZeRO의 핵심 아이디어</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EC%A7%81%EC%A0%91-10b-%EB%AA%A8%EB%8D%B8%EC%9D%84-%ED%95%99%EC%8A%B5%EC%8B%9C%EC%BC%9C-%EB%B3%B8-%EA%B2%BD%ED%97%98\">직접 10B 모델을 학습시켜 본 경험</a></p>\n<ul>\n<li><a href=\"#%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%84%B1\">환경 구성</a></li>\n<li><a href=\"#%EA%B2%AA%EC%97%88%EB%8D%98-%EB%AC%B8%EC%A0%9C%EB%93%A4\">겪었던 문제들</a></li>\n<li><a href=\"#%EC%B2%B4%ED%81%AC%ED%8F%AC%EC%9D%B8%ED%8A%B8-%EA%B4%80%EB%A6%AC%EC%9D%98-%EC%A4%91%EC%9A%94%EC%84%B1\">체크포인트 관리의 중요성</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%ED%98%BC%ED%95%A9-%EC%A0%95%EB%B0%80%EB%8F%84-%ED%95%99%EC%8A%B5-mixed-precision-training\">혼합 정밀도 학습 (Mixed Precision Training)</a></p>\n</li>\n<li>\n<p><a href=\"#2021%EB%85%84%EC%9D%98-%EA%B5%90%ED%9B%88\">2021년의 교훈</a></p>\n<ul>\n<li><a href=\"#%ED%9A%A8%EC%9C%A8%EC%84%B1%EC%9D%B4-%EC%9A%B0%EC%84%A0%EC%9D%B4%EB%8B%A4\">효율성이 우선이다</a></li>\n<li><a href=\"#%EB%B9%85%ED%85%8C%ED%81%AC%EC%99%80%EC%9D%98-%EA%B2%A9%EC%B0%A8\">빅테크와의 격차</a></li>\n<li><a href=\"#%EC%9D%B8%ED%94%84%EB%9D%BC-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81%EC%9D%98-%EC%A4%91%EC%9A%94%EC%84%B1\">인프라 엔지니어링의 중요성</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%A7%88%EC%B9%98%EB%A9%B0\">마치며</a></p>\n</li>\n<li>\n<p><a href=\"#%EC%B0%B8%EA%B3%A0-%EC%9E%90%EB%A3%8C\">참고 자료</a></p>\n</li>\n</ul>\n</li>\n</ul>"}},"pageContext":{"id":"180ae88e-1f04-5e6a-8a1b-968af94badea","frontmatter__slug":"/ai-model-scaling-strategy-2021","previous":"/review-2021","previousTitle":"Review 2021","next":"/remote-work-productivity-tips","nextTitle":"재택근무 1년, 시행착오와 깨달음"}},"staticQueryHashes":["12962592","3399079524","3470099541","4097432363","76375841"],"slicesMap":{}}