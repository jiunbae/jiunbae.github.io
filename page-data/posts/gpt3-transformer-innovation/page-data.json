{"componentChunkName":"component---src-templates-post-tsx","path":"/posts/gpt3-transformer-innovation/","result":{"data":{"markdownRemark":{"html":"<h1 id=\"gpt-3가-바꿔놓은-것들\">GPT-3가 바꿔놓은 것들</h1>\n<p>2020년 6월, OpenAI가 GPT-3를 공개했습니다. 1,750억 개의 파라미터를 가진 거대 언어 모델이었습니다. 숫자만 들으면 \"그래서 뭐가 다른 건데?\"라고 생각할 수 있지만, 이 모델이 AI 연구와 산업에 미친 영향은 생각보다 훨씬 컸습니다.</p>\n<h2 id=\"스케일링이-답이었다\">스케일링이 답이었다</h2>\n<p>GPT-3 이전의 AI 연구는 \"어떤 구조가 더 좋을까\"를 고민하는 것이 대부분이었습니다. LSTM이 좋은지 GRU가 좋은지, Attention을 어디에 붙일지, 어떤 정규화 기법을 쓸지 등 모델 아키텍처에 대한 논의가 주를 이루었습니다.</p>\n<p>그런데 GPT-3는 다른 메시지를 던졌습니다.</p>\n<blockquote>\n<p>\"구조는 그냥 Transformer 쓰고, 대신 크기를 키워라.\"</p>\n</blockquote>\n<p>실제로 GPT-3는 GPT-2와 구조적으로 크게 다르지 않습니다. 동일한 Transformer decoder 아키텍처를 사용하며, 주요 차이점은 규모입니다:</p>\n<table>\n<thead>\n<tr>\n<th>모델</th>\n<th>파라미터 수</th>\n<th>학습 데이터</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>GPT-2</td>\n<td>1.5B</td>\n<td>40GB</td>\n</tr>\n<tr>\n<td>GPT-3</td>\n<td>175B</td>\n<td>570GB</td>\n</tr>\n</tbody>\n</table>\n<p>100배 이상 큰 모델이 100배 이상의 데이터로 학습되었을 뿐인데, 이 단순한 접근이 예상치 못한 능력들을 발현시켰습니다.</p>\n<h2 id=\"in-context-learning과-emergent-abilities\">In-Context Learning과 Emergent Abilities</h2>\n<p>GPT-3에서 가장 인상적이었던 것은 <strong>few-shot learning</strong> 능력입니다. 모델을 별도로 fine-tuning하지 않아도, 프롬프트에 예시를 몇 개 보여주면 새로운 태스크를 수행할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Few-shot learning 예시</span>\nprompt <span class=\"token operator\">=</span> <span class=\"token triple-quoted-string string\">\"\"\"\nTranslate English to French:\nEnglish: Hello, how are you?\nFrench: Bonjour, comment allez-vous?\n\nEnglish: What is your name?\nFrench: Comment vous appelez-vous?\n\nEnglish: I love programming.\nFrench:\n\"\"\"</span>\n\n<span class=\"token comment\"># GPT-3 응답: \"J'aime la programmation.\"</span></code></pre></div>\n<p>프로그래밍 언어 창시자 예시도 흥미로웠습니다:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Python: Guido van Rossum\nJava: James Gosling\nRust: ???</code></pre></div>\n<p>이런 패턴만 보여주면 GPT-3는 \"Graydon Hoare\"라고 답합니다. 별도 학습 없이 패턴을 파악하고 적용하는 것입니다.</p>\n<p>이러한 능력을 **Emergent Ability(창발적 능력)**라고 부릅니다. 모델이 충분히 커지면 명시적으로 가르치지 않은 능력이 나타난다는 개념입니다. 정확히 왜 이런 현상이 발생하는지는 아직 완전히 규명되지 않았지만, 스케일링의 힘을 보여주는 인상적인 사례였습니다.</p>\n<h2 id=\"연구-패러다임의-변화\">연구 패러다임의 변화</h2>\n<p>GPT-3 이후 AI 연구의 트렌드가 크게 바뀌었습니다.</p>\n<h3 id=\"과거-아이디어-중심\">과거: 아이디어 중심</h3>\n<p>예전에는 작은 팀이 좋은 아이디어만 있으면 State-of-the-Art를 달성할 수 있었습니다. <a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a> 논문 하나가 전체 판을 뒤집은 것처럼요. 대학 연구실에서도 충분히 경쟁력 있는 연구가 가능했습니다.</p>\n<h3 id=\"현재-자원-중심\">현재: 자원 중심</h3>\n<p>GPT-3급 모델을 학습시키려면:</p>\n<ul>\n<li><strong>GPU</strong>: 수천 개의 A100 GPU</li>\n<li><strong>비용</strong>: 수백만 달러의 학습 비용</li>\n<li><strong>데이터</strong>: 수백 GB 이상의 고품질 텍스트 데이터</li>\n<li><strong>시간</strong>: 수개월의 학습 시간</li>\n</ul>\n<p>학교 연구실에서 이 규모의 모델을 학습시키는 것은 현실적으로 불가능합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># GPT-3 학습에 필요한 대략적인 계산량</span>\n<span class=\"token comment\"># (이론적 추정)</span>\nparams <span class=\"token operator\">=</span> <span class=\"token number\">175e9</span>  <span class=\"token comment\"># 175B parameters</span>\ntokens <span class=\"token operator\">=</span> <span class=\"token number\">300e9</span>  <span class=\"token comment\"># 300B tokens</span>\nflops_per_token <span class=\"token operator\">=</span> <span class=\"token number\">6</span> <span class=\"token operator\">*</span> params  <span class=\"token comment\"># forward + backward</span>\ntotal_flops <span class=\"token operator\">=</span> flops_per_token <span class=\"token operator\">*</span> tokens\n\n<span class=\"token comment\"># 약 3.14e23 FLOPS</span>\n<span class=\"token comment\"># V100 GPU로 약 355 GPU-년 필요</span></code></pre></div>\n<p>이 변화가 좋은 것인지 나쁜 것인지는 아직 판단하기 어렵습니다. 빅테크만 연구를 주도하게 되는 것 같아 씁쓸한 면이 있지만, 어쨌든 기술 발전은 가속화되고 있습니다.</p>\n<h2 id=\"실무에서의-변화\">실무에서의 변화</h2>\n<p>GPT-3는 실무에도 직접적인 영향을 미쳤습니다. 당시 제 업무에서 느낀 변화들입니다:</p>\n<h3 id=\"1-api로-해결-가능한-영역-확대\">1. API로 해결 가능한 영역 확대</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 2021년 당시 코드 (현재는 deprecated)</span>\n<span class=\"token comment\"># import openai</span>\n<span class=\"token comment\"># response = openai.Completion.create(engine=\"text-davinci-003\", ...)</span>\n\n<span class=\"token comment\"># 현재 OpenAI SDK (v1.0+) 방식</span>\n<span class=\"token keyword\">from</span> openai <span class=\"token keyword\">import</span> OpenAI\n\nclient <span class=\"token operator\">=</span> OpenAI<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">classify_sentiment</span><span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    response <span class=\"token operator\">=</span> client<span class=\"token punctuation\">.</span>chat<span class=\"token punctuation\">.</span>completions<span class=\"token punctuation\">.</span>create<span class=\"token punctuation\">(</span>\n        model<span class=\"token operator\">=</span><span class=\"token string\">\"gpt-4o-mini\"</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># 또는 gpt-3.5-turbo</span>\n        messages<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>\n            <span class=\"token punctuation\">{</span><span class=\"token string\">\"role\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"system\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"content\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Classify the sentiment as positive, negative, or neutral.\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n            <span class=\"token punctuation\">{</span><span class=\"token string\">\"role\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"user\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"content\"</span><span class=\"token punctuation\">:</span> text<span class=\"token punctuation\">}</span>\n        <span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        max_tokens<span class=\"token operator\">=</span><span class=\"token number\">10</span>\n    <span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> response<span class=\"token punctuation\">.</span>choices<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>message<span class=\"token punctuation\">.</span>content<span class=\"token punctuation\">.</span>strip<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 기존에는 모델 학습이 필요했던 작업이 몇 줄로 해결</span></code></pre></div>\n<p>텍스트 분류, 요약, 번역 같은 간단한 NLP 태스크가 API 호출 몇 줄로 해결 가능해졌습니다.</p>\n<h3 id=\"2-ai-엔지니어-역할-변화\">2. \"AI 엔지니어\" 역할 변화</h3>\n<p>AI 엔지니어의 역할이 \"모델 학습하는 사람\"에서 \"프롬프트 잘 짜는 사람\"으로 일부 재정의되기 시작했습니다. 물론 이것은 일부 영역에 한정된 이야기이지만, 분명한 변화였습니다.</p>\n<h3 id=\"3-새로운-집중-영역\">3. 새로운 집중 영역</h3>\n<p>API로 해결되는 문제가 늘어나면서 \"그럼 우리가 뭘 해야 하지?\"라는 고민이 생겼습니다. 결론은 API로 안 되는 것들에 집중하는 것이었습니다:</p>\n<ul>\n<li><strong>도메인 특화</strong>: 특정 분야의 전문 지식이 필요한 태스크</li>\n<li><strong>Latency 최적화</strong>: 실시간 응답이 필요한 서비스</li>\n<li><strong>프라이버시</strong>: 민감한 데이터를 외부 API로 보낼 수 없는 경우</li>\n<li><strong>비용 효율</strong>: 대량 처리 시 API 비용이 부담되는 경우</li>\n</ul>\n<h2 id=\"한계와-문제점\">한계와 문제점</h2>\n<p>물론 GPT-3가 만능은 아니었습니다.</p>\n<h3 id=\"hallucination-환각\">Hallucination (환각)</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Q: 파리는 어느 나라의 수도인가요?\nA: 파리는 독일의 수도입니다.  # 틀린 답을 자신있게 말함</code></pre></div>\n<p>GPT-3는 자신있게 틀린 정보를 생성하는 경우가 많았습니다. 이 문제는 현재의 LLM들도 완전히 해결하지 못했습니다.</p>\n<h3 id=\"일관성-부재\">일관성 부재</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 같은 질문에 다른 답변</span>\nresponse1 <span class=\"token operator\">=</span> gpt3<span class=\"token punctuation\">(</span><span class=\"token string\">\"2 + 2 = ?\"</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># \"4\"</span>\nresponse2 <span class=\"token operator\">=</span> gpt3<span class=\"token punctuation\">(</span><span class=\"token string\">\"2 + 2 = ?\"</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># \"The answer is 4.\"</span>\nresponse3 <span class=\"token operator\">=</span> gpt3<span class=\"token punctuation\">(</span><span class=\"token string\">\"2 + 2 = ?\"</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># \"2 plus 2 equals four.\"</span></code></pre></div>\n<p>Temperature 설정에 따라 같은 질문에도 다른 형식의 답변이 나왔습니다. 프로덕션에서 일관된 결과가 필요할 때 문제가 됐습니다.</p>\n<h3 id=\"비용\">비용</h3>\n<p>2021년 당시 GPT-3 API 비용은 상당했습니다:</p>\n<ul>\n<li>Davinci (가장 성능 좋은 모델): $0.06 / 1K tokens</li>\n<li>대량 처리 시 비용이 빠르게 증가</li>\n</ul>\n<p>지금은 많이 저렴해졌지만, 당시에는 실험하는 것조차 부담스러웠습니다.</p>\n<h2 id=\"scaling-law의-발견\">Scaling Law의 발견</h2>\n<p>GPT-3와 함께 중요한 논문이 발표되었습니다: <a href=\"https://arxiv.org/abs/2001.08361\">Scaling Laws for Neural Language Models</a></p>\n<p>이 논문에서 제시한 핵심 발견:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Loss ∝ N^(-α) · D^(-β) · C^(-γ)\n\nN: 모델 파라미터 수\nD: 학습 데이터 크기\nC: 학습에 사용된 연산량</code></pre></div>\n<p>모델 크기, 데이터, 연산량을 늘리면 <strong>예측 가능한 방식으로</strong> 성능이 향상된다는 것입니다. 이는 앞으로 더 큰 모델을 만들면 어느 정도의 성능 향상을 기대할 수 있는지 예측할 수 있게 해주었습니다.</p>\n<blockquote>\n<p><strong>후속 연구 (2022년 Chinchilla)</strong>: DeepMind의 Chinchilla 논문은 GPT-3가 \"compute-optimal\"하지 않았음을 보여주었습니다. 동일한 연산 예산이라면 모델 크기와 데이터 크기를 균형 있게 늘리는 것이 더 효율적이라는 것입니다. 이후 LLaMA 같은 모델들은 이 교훈을 반영하여 더 적은 파라미터로도 높은 성능을 달성했습니다.</p>\n</blockquote>\n<h2 id=\"지금-돌아보면\">지금 돌아보면</h2>\n<p>2021년의 GPT-3는 시작점이었습니다. 2022년 ChatGPT, 2023년 GPT-4로 이어지면서 당시의 예감이 맞았다는 것이 확인되었습니다.</p>\n<p>그때 \"이거 뭔가 다르다\"라고 느꼈던 것이 틀리지 않았습니다. 다만 이렇게까지 빨리 발전할 줄은 예상하지 못했습니다. 2021년에 GPT-3를 보면서 \"대단하다\"라고 생각했는데, 지금 돌아보면 그것은 정말 시작에 불과했습니다.</p>\n<p>결국 중요한 것은 기술 자체보다 그것을 어떻게 활용하느냐인 것 같습니다. GPT-3가 나왔을 때 \"이걸 어떻게 쓸 수 있을까\"를 빨리 고민한 사람들이 지금 앞서 나가고 있으니까요.</p>\n<h2 id=\"관련-글\">관련 글</h2>\n<ul>\n<li><a href=\"/posts/stable-diffusion-ai-generation-2023\">Stable Diffusion을 실무에 적용해본 경험</a></li>\n<li><a href=\"/posts/stable-diffusion-generative-ai-2023\">SDXL로 텍스처 생성 프로젝트를 진행하면서</a></li>\n<li><a href=\"/posts/llama-opensource-ecosystem-2023\">LLaMA가 바꿔놓은 것들</a></li>\n</ul>\n<hr>\n<h2 id=\"참고-자료\">참고 자료</h2>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2005.14165\">Language Models are Few-Shot Learners</a> - GPT-3 논문</li>\n<li><a href=\"https://arxiv.org/abs/2001.08361\">Scaling Laws for Neural Language Models</a> - 스케일링 법칙 논문</li>\n<li><a href=\"https://platform.openai.com/docs\">OpenAI API Documentation</a> - GPT-3 API 문서</li>\n</ul>","excerpt":"GPT-3가 바꿔놓은 것들 2020년 6월, OpenAI가 GPT-3를 공개했습니다. 1,750억 개의 파라미터를 가진 거대 언어 모델이었습니다. 숫자만 들으면 \"그래서 뭐가 다른 건데?\"라고 생각할 수 있지만, 이 모델이 AI 연구와 산업에 미친 영향은 생각보다 훨씬 컸습니다. 스케일링이 답이었다 GPT-3 이전의 AI 연구는 \"어떤 구조가 더 좋을까\"…","frontmatter":{"date":"21.06.01","dateISO":"2021-06-01","description":"2020년 GPT-3 발표 이후 AI 업계가 어떻게 달라졌는지","slug":"/gpt3-transformer-innovation","heroImage":null,"heroImageAlt":null,"tags":["ai"],"title":"GPT-3가 바꿔놓은 것들"},"tableOfContents":"<ul>\n<li>\n<p><a href=\"#gpt-3%EA%B0%80-%EB%B0%94%EA%BF%94%EB%86%93%EC%9D%80-%EA%B2%83%EB%93%A4\">GPT-3가 바꿔놓은 것들</a></p>\n<ul>\n<li>\n<p><a href=\"#%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81%EC%9D%B4-%EB%8B%B5%EC%9D%B4%EC%97%88%EB%8B%A4\">스케일링이 답이었다</a></p>\n</li>\n<li>\n<p><a href=\"#in-context-learning%EA%B3%BC-emergent-abilities\">In-Context Learning과 Emergent Abilities</a></p>\n</li>\n<li>\n<p><a href=\"#%EC%97%B0%EA%B5%AC-%ED%8C%A8%EB%9F%AC%EB%8B%A4%EC%9E%84%EC%9D%98-%EB%B3%80%ED%99%94\">연구 패러다임의 변화</a></p>\n<ul>\n<li><a href=\"#%EA%B3%BC%EA%B1%B0-%EC%95%84%EC%9D%B4%EB%94%94%EC%96%B4-%EC%A4%91%EC%8B%AC\">과거: 아이디어 중심</a></li>\n<li><a href=\"#%ED%98%84%EC%9E%AC-%EC%9E%90%EC%9B%90-%EC%A4%91%EC%8B%AC\">현재: 자원 중심</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EC%8B%A4%EB%AC%B4%EC%97%90%EC%84%9C%EC%9D%98-%EB%B3%80%ED%99%94\">실무에서의 변화</a></p>\n<ul>\n<li><a href=\"#1-api%EB%A1%9C-%ED%95%B4%EA%B2%B0-%EA%B0%80%EB%8A%A5%ED%95%9C-%EC%98%81%EC%97%AD-%ED%99%95%EB%8C%80\">1. API로 해결 가능한 영역 확대</a></li>\n<li><a href=\"#2-ai-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4-%EC%97%AD%ED%95%A0-%EB%B3%80%ED%99%94\">2. \"AI 엔지니어\" 역할 변화</a></li>\n<li><a href=\"#3-%EC%83%88%EB%A1%9C%EC%9A%B4-%EC%A7%91%EC%A4%91-%EC%98%81%EC%97%AD\">3. 새로운 집중 영역</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%ED%95%9C%EA%B3%84%EC%99%80-%EB%AC%B8%EC%A0%9C%EC%A0%90\">한계와 문제점</a></p>\n<ul>\n<li><a href=\"#hallucination-%ED%99%98%EA%B0%81\">Hallucination (환각)</a></li>\n<li><a href=\"#%EC%9D%BC%EA%B4%80%EC%84%B1-%EB%B6%80%EC%9E%AC\">일관성 부재</a></li>\n<li><a href=\"#%EB%B9%84%EC%9A%A9\">비용</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#scaling-law%EC%9D%98-%EB%B0%9C%EA%B2%AC\">Scaling Law의 발견</a></p>\n</li>\n<li>\n<p><a href=\"#%EC%A7%80%EA%B8%88-%EB%8F%8C%EC%95%84%EB%B3%B4%EB%A9%B4\">지금 돌아보면</a></p>\n</li>\n<li>\n<p><a href=\"#%EA%B4%80%EB%A0%A8-%EA%B8%80\">관련 글</a></p>\n</li>\n<li>\n<p><a href=\"#%EC%B0%B8%EA%B3%A0-%EC%9E%90%EB%A3%8C\">참고 자료</a></p>\n</li>\n</ul>\n</li>\n</ul>"}},"pageContext":{"id":"7d2d5e7e-4b2f-5b6d-bfb5-f60655bfcee3","frontmatter__slug":"/gpt3-transformer-innovation","previous":"/gpt3-transformer-revolution-2021","previousTitle":"GPT-3 API를 처음 써본 날","next":"/technical-research-personnel","nextTitle":"전문연구요원으로 편입하기"}},"staticQueryHashes":["12962592","3399079524","3470099541","4097432363","76375841"],"slicesMap":{}}