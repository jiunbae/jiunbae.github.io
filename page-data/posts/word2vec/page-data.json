{"componentChunkName":"component---src-templates-post-tsx","path":"/posts/word2vec/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"nlp\">NLP</h2>\n<p>NLP(Natural Language Processing)는 다음과 같은 질문에서 출발했습니다.</p>\n<blockquote>\n<p>\"어떻게 하면 컴퓨터가 텍스트를 이해할 수 있을까?\"</p>\n</blockquote>\n<p>사실 아직까지도 컴퓨터는 우리의 머리 속에서 하는 것처럼 텍스트를 이해하지는 못합니다.\n대신 확률과 통계, 그리고 많은 수학을 토대로 비슷하게 작동하도록 만들 수는 있습니다.</p>\n<p>그러면 우리는 어떻게 컴퓨터에게 텍스트의 의미를 알려줄 수 있을까요?</p>\n<blockquote>\n<p>“You shall know a word by the company it keeps”<sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup></p>\n</blockquote>\n<p>물론 많은 방법이 있을 수 있겠지만, 오늘은 문맥을 통해서 텍스트를 이해시키는 방법에 대해 알아보고자 합니다.</p>\n<h2 id=\"word-embedding\">Word Embedding</h2>\n<p>컴퓨터가 텍스트를 이해할 수 있게 하기 위해 가장 먼저 해야할 일은 자연 언어를 수치적인 방식으로 표현하는 것입니다.\n하지만 컴퓨터에게 텍스트의 의미를 알려주는 것은 매우 어려운 일입니다.\n간단한 단어 두개의 개념적 차이와 각각의 의미를 어떻게 알려줄 수 있을까요?</p>\n<p>이전에는 이를 해결하기 위해 one-hot encoding방식을 사용해 왔습니다.\n크기가 n인 단어 사전을 만들고 어떤 단어를 길이가 n인 단어 사전의 해당 단어가 아닌 단어들을 0으로 표시한 벡터로 만드는 방법입니다.\n이런 방식은 단어 간의 관계를 파악하기 힘들고 벡터가 너무 sparse하여 unique한 단어가 증가할 수록 필요로 하는 계산량이 매우 증가하게 됩니다.</p>\n<p>2000년대에 이러한 단점을 개선한 NNLM이 등장합니다.\nNNLM은 어떤 단어 이전의 단어 n개를 one-hot encoding으로 vectorize하여 학습합니다.\n각각 벡터들을 Projection Layer를 통해 Hidden Layer로 전달되고\n이는 Output Layer에서 각 단어들이 나올 확률을 계산하여 실제 단어의 벡터와 비교하여 에러율을 계산하여 weight를 수정해 나갑니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 780px; border-radius: 5px; overflow: hidden;\"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 88.71794871794872%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAABYlAAAWJQFJUiTwAAAC9klEQVR42nVU2Y7aQBDk/38jn5CnaKM8REqiZA8idsmCjY3xbXzfx8y40mNYwFllRMs27i5Xd3X3Av85QgikaYo4jpHnOZIkQdu26Lp2epb/D8PwLm4xfxzpN84Aj8cj8ixDnEbIyhScMWRZiigKCZCdw65xiysjgJFxaexk8j8xOQN5XyGsEnjOEUVZTzFM+nG6judYul+cgeE6ApsvGtTPW2x3HBuFYatwqDsBbQ9sVwnWn57x8JjhZd1A00coKscr+Wp3G2zudjDsG4YSuCwT1FWGtM2JqSB250zo60HsIzja6JoeVVVRHQtiKJB0Ofq6BLp+8r8AMsFQs2ayQQznip7oc8ahOzp0z4BtWnBdl8xB3/fT+3Ko3otSDzU63qGkWo1vRR7fICmoL1H0xexDGMVU32q4krgAyoCe9+hY94/u45lFgbiKKOXm1A0Xo2wpruc3gDLdlrVUNw56hcDscPiuwvqpwXGBgznAdAe8KgmWSx+2w+H4gP1tC0/JiSEjhv0VUNKV6U6p8xph3OJFPSD0WthBg52TUmNT62SAFZYwvIJ6FAjdFlksCLAmMuwKKFOVNZQnamLYlQWOk0PeZ5faydOwmpSNcStmdSvKSFpLQJlyI1r4dYC8KyZHefzKv9wL8hUkhNcEFzApIh/5FdBzqb/2LhRjh4NPzPg4CZEVlBaNnWIfYGkmiqKkPi3h6DZ0Q0eQ0ChSL3LZpLezXOQM6lcLTw8mNvcRiuzk4O8brO7WWG1L7B4C6jlBjAT0xwDPP/ZYPnhwlPI8FOMVUN0p8MMQduJj6BhG6n7e82k68q6k6TjCp/m1LWrk7qRklCVTWWY9+QaoaRpM4wAv98AIcGgGsJaAO0qbZE0p+M/vNR5/PWF5v4TyQgRoDG+FmAHKddTSXvuwXk3zOxX/nIJOa+qjpqKiq2RqOc60Ur5YBp4Cb+Y7Gz3Xc7HRdcQRLdOMFgPtMEGB2t6ATrObUhMydlKybWq8qjuYBF6QKELMRfkLG85q+S+RL3gAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"NNLM\"\n        title=\"\"\n        src=\"/static/6bffbfb2a42fee5467f45fd0f3a55b7a/a1792/NNLM.png\"\n        srcset=\"/static/6bffbfb2a42fee5467f45fd0f3a55b7a/cb9a0/NNLM.png 195w,\n/static/6bffbfb2a42fee5467f45fd0f3a55b7a/727ba/NNLM.png 390w,\n/static/6bffbfb2a42fee5467f45fd0f3a55b7a/a1792/NNLM.png 780w,\n/static/6bffbfb2a42fee5467f45fd0f3a55b7a/105d8/NNLM.png 1170w,\n/static/6bffbfb2a42fee5467f45fd0f3a55b7a/9f9a4/NNLM.png 1560w,\n/static/6bffbfb2a42fee5467f45fd0f3a55b7a/3e096/NNLM.png 1718w\"\n        sizes=\"(max-width: 780px) 100vw, 780px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>이는 기본적으로 <a href=\"https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis\">Distributional Hypothesis</a>을 기반으로 이루어 집니다. 비슷한 분포를 가지는 단어가 비슷한 의미를 가질 것 이므로, 학습된 NNLM은 비슷한 단어를 찾아내고 의미를 파악하는데 도움을 줄 수 있습니다.</p>\n<p>NNLM은 단어의 벡터화라는 새로운 패러다임을 이끌었지만 몇 가지 단점이 있었습니다.</p>\n<ul>\n<li>고정된  n값</li>\n<li>단어 이후에 나오는 단어를 고려하지 못함.</li>\n<li>학습 속도가 매우 느림</li>\n</ul>\n<p><a href=\"https://arxiv.org/abs/1301.3781\">Efficient Estimation of Word Representations in Vector Space</a> <sup id=\"fnref-2\"><a href=\"#fn-2\" class=\"footnote-ref\">2</a></sup> 에서는 Word2vec을 소개하며 이러한 단점들을 극복하고 특히 학습 속도를 매우 개선하였습니다.</p>\n<p>Word2vec은 CBOW(continuous bag-of-words)와 Skip-grams이라는 두 가지 모델 아키텍쳐를 제시하였습니다.</p>\n<p>CBOW모델은 주어진 단어 앞뒤의 단어를 Input으로 주어진 단어를 맞추기 위해 학습하는 네트워크를 구성하고,\nSkip-gram 모델은 단어 하나를 가지고 주위의 단어가 나올 확률을 추론합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 780px; border-radius: 5px; overflow: hidden;\"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 59.48717948717949%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAABD0lEQVR42p2SyQ6DMAxE8/+fxw04wY193xe3z1JQiqpKZQ5R4sQzYztG3iiKQqZpkr7vJUkSKctSjuOQNE1lGAaxIJbnuWzbJlVV6X3bttJ1neYAAxFJ67rKU8zzrIbgMCwo1HWtDp4gyzJ1CKlZlkVLhZTAHed5fiWxcXIwM46jOjUsHCibvqBGzPbsTujG6CXOmAGmlNBV49L3fcE1zebcNI0mAkSphGTEGRBmXCjhvu+qjLs4ji8nPHaHBTEl2veIUa49KyFOCKLM3h0Mzu8lu2f25GAEx+yNbaxb2j+gAohoEwLqEHb3Az/5NvQTU8b25ukftDOAQx3e/5TneddgfhGEYShBEEgURR9TfgEw+az2Npvq1wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"CBOW and Skip-gram\"\n        title=\"\"\n        src=\"/static/1bef0c82c6ff7b45833001b9b24b179b/a1792/cbow-skip-gram.png\"\n        srcset=\"/static/1bef0c82c6ff7b45833001b9b24b179b/cb9a0/cbow-skip-gram.png 195w,\n/static/1bef0c82c6ff7b45833001b9b24b179b/727ba/cbow-skip-gram.png 390w,\n/static/1bef0c82c6ff7b45833001b9b24b179b/a1792/cbow-skip-gram.png 780w,\n/static/1bef0c82c6ff7b45833001b9b24b179b/105d8/cbow-skip-gram.png 1170w,\n/static/1bef0c82c6ff7b45833001b9b24b179b/dd507/cbow-skip-gram.png 1528w\"\n        sizes=\"(max-width: 780px) 100vw, 780px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>여기에 Output Layer에서 Softmax를 계산할 때 Huffman Tree를 이용한 Hierarchical Softmax와\nNegative Sampling등의 방법을 이용해서 효과적으로 연산량을 줄였습니다.</p>\n<p><a href=\"https://radimrehurek.com/gensim/models/word2vec.html\">genism</a>이나 <a href=\"https://www.tensorflow.org/tutorials/word2vec\">tensorflow</a>등, 구체화된 훌륭한 라이브러리들을 많이 찾을 수 있기 때문에 살펴보는 것을 권장합니다.</p>\n<h2 id=\"응용\">응용</h2>\n<p>Word2vec을 이용하여 단어를 vector space에 mapping할 수 있습니다.\nVector space에 mapping된 vector들은 각 단어의 특성을 비슷하게 가지고 있습니다.\n각 벡터의 차이는 실제 단어의 의미상 차이와 유사해 지고, 비슷한 벡터는 의미적으로 비슷할 확률이 높습니다.\n예를 들어 <code class=\"language-text\">한국 – 서울 + 도쿄</code>라는 질의에 대해 <code class=\"language-text\">일본</code>을 반환해 줄 것입니다.</p>\n<h3 id=\"데이터-수집\">데이터 수집</h3>\n<p>우선 모델을 학습할 데이터를 모으는게 우선입니다.\n주변에서 쉽게 찾을 수 있는 풍부한 한국어 데이터로는 나무위키, 한국어 위키피디아, 한국어 공개 코퍼스 등이 있습니다.</p>\n<p>혹은 책이나 뉴스 기사, 잡지, 연설문 등으로 해도 재밌는 결과가 나오며 실제 텍스트를 분석하는데 도움이 됩니다.\n<del>심지어 코드도!!</del></p>\n<h3 id=\"전처리\">전처리</h3>\n<p>한국어의 경우 형태소 분석기를 통해 텍스트를 형태소 단위로 나누어 줘야 합니다. 띄어쓰기 단위로 단어만 분리하면 정확도가 높지 않습니다.\n<a href=\"http://konlpy.org/ko/latest/\">KoNLPy</a>나 <a href=\"http://eunjeon.blogspot.kr/\">은전한닢</a> 같은 훌륭한 라이브러리들이 많이 있어 어렵지 않게 사용할 수 있습니다.\n불용어도 제거해 주는 것이 성능을 향상시키는 데에 도움이 됩니다.</p>\n<p>이 부분은 어떤 형태소 분석기를 사용하더라도 완벽하게 형태소가 분리되기 어렵고,\n형태소가 잘 나뉘어져 있다고 해도 어순에 의해 별로 연관되지 않은 단어가 연관되게 나타날 수 도 있습니다.\n때문에 휴리스틱한 부분이 대다수 여서 딱히 정해진 규칙이나, 일반적인 방법을 찾아내기 힘듭니다.\n자신의 데이터에 맞는 처리 방법을 찾아내는 것이 가장 좋습니다.</p>\n<p>나무위키나 위키피디아의 경우 redirect나 틀, 혹은 여러 포맷들을 수정해 주어야합니다.</p>\n<h3 id=\"학습시키기\">학습시키기</h3>\n<p>gensim에서 제공해주는 word2vec모델을 사용하면 손쉽게 학습시킬 수 있습니다.\n<a href=\"https://radimrehurek.com/gensim/models/word2vec.html\">여기</a>를 참고해 보세요.</p>\n<h3 id=\"데모-만들기\">데모 만들기</h3>\n<p>웹이나 앱으로 만들어진 모델 파일을 이용해 조금만 처리를 하면 손쉽게 만들어 볼 수 있습니다.\n실제 만들어진 <a href=\"http:/server2.memento.live:5000\">Demo</a>를 구경해 보실 수 있습니다. 데모의 <a href=\"https://github.com/memento7/word2demo\">소스코드</a>도 공개되어 있습니다.</p>\n<h2 id=\"project-memento\"><a href=\"https://memento7.github.io/2017/word2vec/\">Project: memento</a></h2>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-1\">Firth, J. R. 1957:11<a href=\"#fnref-1\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-2\">Mikolov, Tomas; et al. \"Efficient Estimation of Word Representations in Vector Space\". 16 Jan 2013<a href=\"#fnref-2\" class=\"footnote-backref\">↩</a></li>\n</ol>\n</div>","excerpt":"NLP NLP(Natural Language Processing)는 다음과 같은 질문에서 출발했습니다. \"어떻게 하면 컴퓨터가 텍스트를 이해할 수 있을까?\" 사실 아직까지도 컴퓨터는 우리의 머리 속에서 하는 것처럼 텍스트를 이해하지는 못합니다.\n대신 확률과 통계, 그리고 많은 수학을 토대로 비슷하게 작동하도록 만들 수는 있습니다. 그러면 우리는 어떻게 컴…","frontmatter":{"date":"17.04.27","description":"word embedding for clustering","heroImage":null,"heroImageAlt":null,"tags":["nlp","tech"],"title":"Word Embedding"},"tableOfContents":"<ul>\n<li>\n<p><a href=\"#nlp\">NLP</a></p>\n</li>\n<li>\n<p><a href=\"#word-embedding\">Word Embedding</a></p>\n</li>\n<li>\n<p><a href=\"#%EC%9D%91%EC%9A%A9\">응용</a></p>\n<ul>\n<li><a href=\"#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91\">데이터 수집</a></li>\n<li><a href=\"#%EC%A0%84%EC%B2%98%EB%A6%AC\">전처리</a></li>\n<li><a href=\"#%ED%95%99%EC%8A%B5%EC%8B%9C%ED%82%A4%EA%B8%B0\">학습시키기</a></li>\n<li><a href=\"#%EB%8D%B0%EB%AA%A8-%EB%A7%8C%EB%93%A4%EA%B8%B0\">데모 만들기</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#project-memento\">Project: memento</a></p>\n</li>\n</ul>"}},"pageContext":{"id":"a039d83b-9170-54ea-bfa5-ddb44a5c9dc7","frontmatter__slug":"/word2vec","previous":"/keyword-extraction","previousTitle":"Keyword Extraction","next":"/MAT4015","nextTitle":"MAT4015: Fundamentals of applied probability and random processes"}},"staticQueryHashes":["12962592","3399079524","3470099541","76375841"],"slicesMap":{}}